# Embedding Service Optimization Recommendations
## Feature 2.1: Actionable Optimization Guide

**Date**: October 31, 2025
**Feature**: 2.1 - Embedding Service Profiling
**Status**: COMPLETE
**Priority**: HIGH for Feature 2.4 Implementation

---

## Executive Summary

This document provides prioritized, actionable optimization recommendations based on comprehensive profiling of the EmbeddingService. Each recommendation includes implementation details, expected impact, effort estimate, and priority level.

**Top 3 Recommendations** (Quick Wins):
1. ‚≠ê **Increase default batch size to 64** - 13% throughput improvement (5 min)
2. ‚≠ê **Implement text truncation to 256 chars** - 40-60% improvement for long texts (15 min)
3. ‚≠ê **Add memory-aware batch sizing** - Better resource utilization (30 min)

**Total Expected Impact**: 15-25% overall throughput improvement with <1 hour implementation time

---

## Optimization Catalog

### Quick Wins (High Impact, Low Effort)

---

#### Optimization 1: Increase Default Batch Size

**Priority**: üî¥ **CRITICAL - Implement Immediately**
**Effort**: ‚≠ê Very Low (5 minutes)
**Expected Impact**: +13% throughput at batch_size=64, +26% at batch_size=256
**Risk**: Very Low
**Prerequisites**: None

**Current State**:
```python
class EmbeddingService:
    DEFAULT_BATCH_SIZE = 32
```

**Problem**:
- Current default (32) is conservative
- Profiling shows optimal performance at 64-256
- Missing 13-26% potential throughput

**Recommended Change**:
```python
class EmbeddingService:
    # Updated based on Feature 2.1 profiling (Oct 31, 2025)
    # Previous: 32 (conservative default)
    # Profiling results:
    #   - batch_size=32:  1,018 texts/sec
    #   - batch_size=64:  1,341 texts/sec (+13%)
    #   - batch_size=128: 1,478 texts/sec (+18%)
    #   - batch_size=256: 1,493 texts/sec (+26%)
    DEFAULT_BATCH_SIZE = 64  # Recommended for balanced performance
```

**Implementation Steps**:
1. Update `DEFAULT_BATCH_SIZE` constant
2. Update docstrings mentioning default
3. Run existing tests to confirm no regression
4. Update documentation

**Verification**:
```python
# Test performance with new default
service = EmbeddingService.get_instance()
texts = load_test_texts(1000)

start = time.perf_counter()
embeddings = service.embed_batch(texts)  # Uses DEFAULT_BATCH_SIZE
elapsed = time.perf_counter() - start

throughput = len(texts) / elapsed
assert throughput > 1300, f"Expected >1300 texts/sec, got {throughput}"
```

**Expected Results**:
- Throughput increase from ~1,018 to ~1,341 texts/sec
- Memory increase from ~485 MB to ~510 MB (5% increase)
- No code changes required for existing users
- Seamless upgrade for all downstream services

**Rollback Plan**:
If issues arise, simply revert to `DEFAULT_BATCH_SIZE = 32`

**Monitoring**:
- Track average throughput before/after change
- Monitor P95 latency
- Watch memory usage (should stay <600 MB)

---

#### Optimization 2: Text Length Truncation

**Priority**: üî¥ **CRITICAL - Implement Immediately**
**Effort**: ‚≠ê‚≠ê Low (15 minutes)
**Expected Impact**: +40-60% throughput for texts >256 chars
**Risk**: Low (minimal information loss for claim verification)
**Prerequisites**: None

**Current State**:
- No text length limits
- Long texts (>512 chars) cause 89% performance drop
- Average claim length: 90 chars, but some outliers >1000 chars

**Problem**:
- Text length strongly correlates with processing time (r=-0.88)
- 1024-char texts: 132 texts/sec (89% slower than optimal)
- 256-char texts: 482 texts/sec (60% slower than optimal)
- Diminishing semantic returns beyond 256 characters

**Recommended Solution**:
Add preprocessing step to truncate texts to 256 characters:

```python
# File: truthgraph/services/ml/embedding_service.py

class EmbeddingService:
    # Configuration
    MAX_TEXT_LENGTH = 256  # Based on Feature 2.1 profiling
    ENABLE_TRUNCATION = True

    @staticmethod
    def _preprocess_text(text: str, max_length: int = MAX_TEXT_LENGTH) -> str:
        """Preprocess text before embedding generation.

        Args:
            text: Input text
            max_length: Maximum length in characters

        Returns:
            Preprocessed text (truncated if necessary)
        """
        if not text or not EmbeddingService.ENABLE_TRUNCATION:
            return text

        if len(text) <= max_length:
            return text

        # Truncate at sentence boundary to preserve meaning
        truncated = text[:max_length]

        # Try to cut at last sentence
        last_period = truncated.rfind('.')
        if last_period > max_length * 0.7:  # At least 70% of text
            return truncated[:last_period + 1]

        # Fallback: cut at last space
        last_space = truncated.rfind(' ')
        if last_space > 0:
            return truncated[:last_space]

        return truncated

    def embed_text(self, text: str) -> list[float]:
        """Generate embedding for a single text."""
        if not text or not isinstance(text, str):
            raise ValueError("Text must be a non-empty string")

        # Preprocess text
        processed_text = self._preprocess_text(text)

        # Ensure model is loaded
        self._load_model()

        # ... rest of implementation
```

**Implementation Steps**:
1. Add `_preprocess_text` method to EmbeddingService
2. Add `MAX_TEXT_LENGTH` and `ENABLE_TRUNCATION` class constants
3. Call preprocessing in `embed_text` and `embed_batch`
4. Add tests for truncation behavior
5. Update documentation

**Testing**:
```python
def test_text_truncation():
    """Test that long texts are truncated correctly."""
    service = EmbeddingService.get_instance()

    # Test long text
    long_text = "A" * 1000
    embedding = service.embed_text(long_text)

    # Verify embedding generated successfully
    assert len(embedding) == 384

    # Test that truncation preserves sentence boundaries
    text_with_sentences = "First sentence. " * 100  # ~1500 chars
    embedding = service.embed_text(text_with_sentences)
    assert len(embedding) == 384

def test_truncation_disable():
    """Test that truncation can be disabled."""
    service = EmbeddingService.get_instance()
    service.ENABLE_TRUNCATION = False

    long_text = "A" * 1000
    embedding = service.embed_text(long_text)

    # Should still work, just slower
    assert len(embedding) == 384

    service.ENABLE_TRUNCATION = True  # Restore default
```

**Expected Results**:
- Texts >256 chars: 40-60% throughput improvement
- Texts <256 chars: No change
- Average throughput across mixed workload: 15-20% improvement
- Minimal semantic information loss

**Performance Comparison**:
| Text Length | Before (texts/sec) | After (texts/sec) | Improvement |
|-------------|-------------------|-------------------|-------------|
| 256 chars   | 482               | 1,018 (est)       | +111% |
| 512 chars   | 273               | 1,018 (est)       | +273% |
| 1024 chars  | 132               | 1,018 (est)       | +671% |

**Risk Mitigation**:
- Truncation at sentence boundaries preserves semantic coherence
- For claim verification, first 256 chars typically contain core claim
- Can be disabled via `ENABLE_TRUNCATION = False` if needed
- Log truncated text lengths for monitoring

---

#### Optimization 3: Memory-Aware Batch Sizing

**Priority**: üü° **HIGH - Implement in Feature 2.4**
**Effort**: ‚≠ê‚≠ê Medium (30 minutes)
**Expected Impact**: 5-10% throughput improvement for variable workloads
**Risk**: Low
**Prerequisites**: Optimization 1 and 2

**Current State**:
- Fixed batch size regardless of memory availability
- No adaptation to workload characteristics
- Some scenarios over/under-utilize resources

**Problem**:
- Memory-constrained environments can't use large batches
- High-memory systems underutilize capacity
- Variable text lengths need different batch sizes

**Recommended Solution**:
Implement adaptive batch sizing based on available memory and text characteristics:

```python
# File: truthgraph/services/ml/embedding_service.py

import psutil

class EmbeddingService:
    # Memory thresholds (MB)
    MEMORY_THRESHOLD_HIGH = 1024  # >1GB available
    MEMORY_THRESHOLD_MEDIUM = 512  # 512MB-1GB available
    MEMORY_THRESHOLD_LOW = 256     # <512MB available

    def _get_adaptive_batch_size(
        self,
        num_texts: int,
        avg_text_length: int | None = None,
    ) -> int:
        """Determine optimal batch size based on system resources and workload.

        Args:
            num_texts: Number of texts to process
            avg_text_length: Average text length (chars), if known

        Returns:
            Optimal batch size for this workload
        """
        # Check available memory
        mem = psutil.virtual_memory()
        available_mb = mem.available / 1024 / 1024

        # Base batch size on available memory
        if available_mb > self.MEMORY_THRESHOLD_HIGH:
            base_batch = 256
        elif available_mb > self.MEMORY_THRESHOLD_MEDIUM:
            base_batch = 128
        elif available_mb > self.MEMORY_THRESHOLD_LOW:
            base_batch = 64
        else:
            base_batch = 32

        # Adjust for text length if known
        if avg_text_length:
            if avg_text_length > 512:
                base_batch = min(base_batch, 32)
            elif avg_text_length > 256:
                base_batch = min(base_batch, 64)

        # Adjust for small workloads
        if num_texts < base_batch:
            return max(8, num_texts)

        return base_batch

    def embed_batch(
        self,
        texts: list[str],
        batch_size: int | None = None,
        show_progress: bool = False,
        adaptive: bool = True,
    ) -> list[list[float]]:
        """Generate embeddings with adaptive batch sizing.

        Args:
            texts: List of texts to embed
            batch_size: Fixed batch size (if None and adaptive=True, uses adaptive)
            show_progress: Show progress bar
            adaptive: Use adaptive batch sizing if batch_size is None
        """
        # ... validation ...

        # Determine batch size
        if batch_size is None:
            if adaptive:
                # Calculate average text length
                avg_length = sum(len(t) for t in texts) / len(texts)
                batch_size = self._get_adaptive_batch_size(
                    len(texts),
                    int(avg_length)
                )
                logger.info(f"Using adaptive batch_size={batch_size}")
            else:
                batch_size = self.DEFAULT_BATCH_SIZE

        # ... rest of implementation ...
```

**Implementation Steps**:
1. Add `_get_adaptive_batch_size` method
2. Add memory threshold constants
3. Modify `embed_batch` to support adaptive sizing
4. Add `adaptive` parameter (default True)
5. Add tests for different memory scenarios
6. Document adaptive behavior

**Expected Results**:
- Memory-constrained: Automatically use smaller batches
- High-memory: Automatically use larger batches
- Variable workloads: Optimal performance for each batch
- 5-10% improvement in heterogeneous environments

**Monitoring**:
- Log selected batch sizes
- Track batch size distribution
- Monitor memory utilization
- Alert if frequently hitting memory limits

---

### Medium-Term Improvements (Medium Effort, Medium-High Impact)

---

#### Optimization 4: GPU Acceleration

**Priority**: üü¢ **MEDIUM - Test and Document**
**Effort**: ‚≠ê‚≠ê‚≠ê Medium (2-3 hours)
**Expected Impact**: 2-5x throughput improvement
**Risk**: Low (CPU fallback already works)
**Prerequisites**: CUDA-compatible GPU, PyTorch with CUDA

**Current State**:
- GPU detection code exists
- Tested only on CPU
- GPU performance unverified

**Recommended Actions**:
1. **Test with CUDA GPU**:
   - Run all three profiling scripts with GPU
   - Compare CPU vs GPU performance
   - Identify optimal batch sizes for GPU

2. **Document GPU Performance**:
   - Create GPU performance benchmarks
   - Document recommended settings
   - Add GPU troubleshooting guide

3. **Optimize for GPU**:
   - Test batch sizes 128-512 (GPUs prefer larger batches)
   - Measure GPU memory usage
   - Identify GPU memory limits

**Expected Results** (estimated):
- Throughput: 3,000-7,000 texts/sec (vs 1,500 on CPU)
- Optimal batch size: 256-512 (vs 64-128 on CPU)
- GPU memory usage: 1-2 GB
- Latency: 0.2-0.4 ms/text (vs 0.7 ms on CPU)

**Implementation** (if GPU available):
```python
# Already implemented in codebase:
# - Device detection (CUDA/MPS/CPU)
# - Automatic device selection
# - GPU memory cleanup

# No code changes needed, just testing and documentation
```

---

#### Optimization 5: Embedding Cache

**Priority**: üü¢ **MEDIUM - Feature 2.5+**
**Effort**: ‚≠ê‚≠ê‚≠ê‚≠ê High (4-8 hours)
**Expected Impact**: 50-90% improvement for repeated texts
**Risk**: Medium (cache invalidation, memory overhead)
**Prerequisites**: None

**Use Case**:
- Repeated claim verification
- Common evidence documents
- Frequently queried texts

**Recommended Implementation**:
```python
from functools import lru_cache
import hashlib

class EmbeddingService:
    def __init__(self):
        # ... existing init ...
        self._cache_enabled = True
        self._cache_size = 10000  # LRU cache size
        self._cache_hits = 0
        self._cache_misses = 0

    def _cache_key(self, text: str) -> str:
        """Generate cache key for text."""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    @lru_cache(maxsize=10000)
    def _embed_cached(self, text_hash: str, text: str) -> tuple[float, ...]:
        """Cached embedding generation (internal use)."""
        embedding = self._embed_uncached(text)
        return tuple(embedding)  # Tuples are hashable for cache

    def embed_text(self, text: str, use_cache: bool = True) -> list[float]:
        """Generate embedding with optional caching."""
        processed_text = self._preprocess_text(text)

        if use_cache and self._cache_enabled:
            text_hash = self._cache_key(processed_text)
            embedding_tuple = self._embed_cached(text_hash, processed_text)
            self._cache_hits += 1
            return list(embedding_tuple)
        else:
            self._cache_misses += 1
            return self._embed_uncached(processed_text)

    def get_cache_stats(self) -> dict:
        """Get cache performance statistics."""
        total = self._cache_hits + self._cache_misses
        hit_rate = self._cache_hits / total if total > 0 else 0

        return {
            "cache_enabled": self._cache_enabled,
            "cache_size": self._cache_size,
            "cache_hits": self._cache_hits,
            "cache_misses": self._cache_misses,
            "hit_rate": hit_rate,
        }
```

**Expected Results**:
- Cache hit rate: 20-60% (depends on workload)
- Throughput for cached texts: Near-instant
- Memory overhead: ~40 MB per 10,000 cached embeddings
- Overall throughput improvement: 10-30% for typical workloads

---

#### Optimization 6: Parallel Processing

**Priority**: üü¢ **LOW - Future Work**
**Effort**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very High (multiple days)
**Expected Impact**: Near-linear scaling with workers
**Risk**: High (complexity, synchronization, resource management)
**Prerequisites**: Advanced requirements only

**Use Case**:
- Very high throughput requirements (>10,000 texts/sec)
- Multiple GPUs available
- Distributed system architecture

**Recommended Approach**:
- Multi-process workers with queue-based distribution
- Load balancing across multiple GPUs
- Result aggregation and ordering

**Note**: This is a significant architectural change and should only be considered if single-instance performance (1,500-7,000 texts/sec) is insufficient.

---

## Implementation Priority Matrix

### Priority 1: Immediate (Feature 2.4)
| Optimization | Effort | Impact | Status |
|--------------|--------|--------|--------|
| Increase default batch size | 5 min | +13% throughput | üî¥ TODO |
| Text truncation | 15 min | +40-60% for long texts | üî¥ TODO |
| Memory-aware batching | 30 min | +5-10% variable workloads | üü° TODO |

**Total Time**: ~1 hour
**Expected Combined Impact**: +15-25% overall throughput

### Priority 2: Short-Term (Feature 2.5)
| Optimization | Effort | Impact | Status |
|--------------|--------|--------|--------|
| GPU testing | 2-3 hours | 2-5x throughput | üü¢ OPTIONAL |
| Embedding cache | 4-8 hours | +10-30% repeated texts | üü¢ OPTIONAL |

### Priority 3: Long-Term (Phase 3+)
| Optimization | Effort | Impact | Status |
|--------------|--------|--------|--------|
| Model optimization | Days | +10-30% | üîµ FUTURE |
| Parallel processing | Days | Near-linear scaling | üîµ FUTURE |

---

## Testing Strategy

### Unit Tests
```python
# tests/test_profiling.py

def test_default_batch_size():
    """Verify default batch size is optimal."""
    assert EmbeddingService.DEFAULT_BATCH_SIZE == 64

def test_text_truncation():
    """Verify long texts are truncated."""
    service = EmbeddingService.get_instance()
    long_text = "A" * 1000
    embedding = service.embed_text(long_text)
    assert len(embedding) == 384

def test_adaptive_batch_sizing():
    """Verify adaptive batch sizing works."""
    service = EmbeddingService.get_instance()

    # Small workload
    texts = ["test"] * 10
    # Should use small batch

    # Large workload
    texts = ["test"] * 10000
    # Should use large batch based on memory
```

### Performance Tests
```python
def test_performance_no_regression():
    """Verify optimizations don't regress performance."""
    service = EmbeddingService.get_instance()
    texts = load_test_texts(1000)

    start = time.perf_counter()
    embeddings = service.embed_batch(texts)
    elapsed = time.perf_counter() - start

    throughput = len(texts) / elapsed

    # Should exceed baseline
    assert throughput > 1300, f"Regression: {throughput} texts/sec"

def test_memory_stability():
    """Verify memory usage is stable."""
    service = EmbeddingService.get_instance()
    texts = load_test_texts(100)

    baseline = get_memory_mb()

    for _ in range(10):
        service.embed_batch(texts)

    final = get_memory_mb()
    growth = final - baseline

    # Memory growth should be minimal
    assert growth < 50, f"Memory leak: {growth} MB growth"
```

---

## Monitoring and Validation

### Metrics to Track

**Before Optimization**:
- Throughput: ~1,018 texts/sec (batch=32)
- P95 latency: ~1.0 ms/text
- Memory: ~485 MB peak

**After Optimization** (Expected):
- Throughput: ~1,341 texts/sec (batch=64) [+32%]
- With truncation: ~1,500 texts/sec (mixed workload) [+47%]
- P95 latency: ~0.75 ms/text
- Memory: ~530 MB peak

### Success Criteria

‚úÖ **Optimization Successful If**:
- Throughput increase >10%
- P95 latency decrease or stable
- Memory increase <20%
- No new errors or failures
- All tests passing

‚ùå **Rollback If**:
- Throughput decrease >5%
- P95 latency increase >20%
- Memory increase >50%
- Error rate increase >1%
- Test failures

---

## Risk Mitigation

### Risk 1: Performance Regression
**Mitigation**: Comprehensive before/after testing, gradual rollout

### Risk 2: Memory Issues
**Mitigation**: Memory monitoring, adaptive batch sizing, rollback plan

### Risk 3: Text Truncation Impact
**Mitigation**: Truncate at sentence boundaries, make configurable, A/B test

### Risk 4: Configuration Errors
**Mitigation**: Sensible defaults, validation, documentation

---

## Rollout Plan

### Phase 1: Quick Wins (Week 1)
1. Deploy optimization 1 (batch size) to staging
2. Monitor for 24 hours
3. Deploy to production
4. Deploy optimization 2 (truncation) to staging
5. Monitor for 24 hours
6. Deploy to production

### Phase 2: Adaptive Batching (Week 2)
1. Implement and test thoroughly
2. Deploy to staging
3. Monitor for 48 hours
4. Gradual rollout to production (10% -> 50% -> 100%)

### Phase 3: GPU Testing (Week 3+)
1. Test in GPU environment
2. Document findings
3. Provide GPU deployment guide
4. Optional GPU rollout

---

## Conclusion

These optimization recommendations provide a clear path to **15-25% throughput improvement** with minimal effort. The quick wins (batch size and text truncation) can be implemented in under 1 hour and provide immediate, measurable benefits.

**Recommended Action Plan for Feature 2.4**:
1. ‚úÖ Implement Optimization 1 (5 min)
2. ‚úÖ Implement Optimization 2 (15 min)
3. ‚úÖ Implement Optimization 3 (30 min)
4. ‚úÖ Test and validate (30 min)
5. ‚úÖ Deploy to staging and production

**Total Implementation Time**: ~1.5 hours
**Expected Throughput Improvement**: +15-25%
**Risk Level**: Low
**Production Readiness**: High

---

**Document Status**: COMPLETE
**Next Steps**: Feature 2.4 Implementation
**Date**: October 31, 2025
